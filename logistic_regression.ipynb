{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/roxyrong/w281-project/blob/main/logistic_regression.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pmww0qJycIFk",
        "outputId": "e74f46a1-2a86-4873-aae6-f90a5ed8af2a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "otWK_RlFkmNM",
        "outputId": "93c70f1d-06a3-4b30-e9ea-1b33ffb3ddf3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 428 µs (started: 2023-12-11 10:55:55 +00:00)\n"
          ]
        }
      ],
      "source": [
        "%%capture\n",
        "!pip install ipython-autotime\n",
        "%load_ext autotime"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J8ewb_JS-IB8",
        "outputId": "a6c8ef59-2574-4808-d534-0cd5abd4667e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 1.7 s (started: 2023-12-11 10:55:55 +00:00)\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import time\n",
        "import copy\n",
        "from glob import glob\n",
        "import numpy as np\n",
        "from numpy import fft\n",
        "import pandas as pd\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from PIL import Image\n",
        "\n",
        "from sklearn import metrics\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.metrics import make_scorer, accuracy_score, precision_score, recall_score, f1_score, classification_report\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W18nRczIcTSZ"
      },
      "source": [
        "# Data Loading & Cleaning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v7qCB9dG7A8z",
        "outputId": "444ddd6d-dcea-4d3b-c38a-70dfba207591"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 35.8 s (started: 2023-12-11 10:55:56 +00:00)\n"
          ]
        }
      ],
      "source": [
        "def load_dataset_parquet(datapath):\n",
        "    df = pd.read_parquet(datapath)\n",
        "    for idx, row in df.iterrows():\n",
        "      df.at[idx, \"image\"] = df.at[idx, \"image\"].reshape((150, 150, 3)).astype(\"uint8\")\n",
        "    return df\n",
        "\n",
        "\n",
        "train_path = \"drive/MyDrive/github/w281-project-me/dataset/train_features_full.parquet.gzip\"\n",
        "test_path = \"drive/MyDrive/github/w281-project-me/dataset/test_features_full.parquet.gzip\"\n",
        "train_df = load_dataset_parquet(train_path)\n",
        "test_df = load_dataset_parquet(test_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2YgQ8xHW-mQq",
        "outputId": "9da1e1e8-e95a-4d47-fab7-ad99da88bd67"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 7.17 ms (started: 2023-12-11 10:56:32 +00:00)\n"
          ]
        }
      ],
      "source": [
        "train_df, valid_df = train_test_split(train_df, test_size=0.2, random_state = 314)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dv_5gkYIw_9P",
        "outputId": "97970036-a63f-42ee-8cea-734613340ac1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "category\n",
            "buildings    1658\n",
            "forest       1773\n",
            "glacier      1871\n",
            "mountain     1970\n",
            "sea          1762\n",
            "street       1653\n",
            "Name: image, dtype: int64\n",
            "category\n",
            "buildings    435\n",
            "forest       423\n",
            "glacier      495\n",
            "mountain     503\n",
            "sea          425\n",
            "street       391\n",
            "Name: image, dtype: int64\n",
            "category\n",
            "buildings    414\n",
            "forest       450\n",
            "glacier      544\n",
            "mountain     520\n",
            "sea          481\n",
            "street       444\n",
            "Name: image, dtype: int64\n",
            "time: 10 ms (started: 2023-12-11 10:56:32 +00:00)\n"
          ]
        }
      ],
      "source": [
        "print(train_df.groupby(\"category\")[\"image\"].count())\n",
        "print(valid_df.groupby(\"category\")[\"image\"].count())\n",
        "print(test_df.groupby(\"category\")[\"image\"].count())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nkti-ccHvVbz"
      },
      "source": [
        "# Modeling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VAVDew_XDRkt",
        "outputId": "2d33bd38-f4be-4b60-94f6-f00111ef60ea"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 590 µs (started: 2023-12-11 10:56:32 +00:00)\n"
          ]
        }
      ],
      "source": [
        "features_count = {\"mean_h\": 1,\n",
        "                 \"mean_s\": 1,\n",
        "                 \"mean_v\": 1,\n",
        "                 \"stddev_h\": 1,\n",
        "                 \"stddev_s\": 1,\n",
        "                 \"stddev_v\": 1,\n",
        "                 \"pca_hsv_histogram\" : 50,\n",
        "                  \"pca_hog_feature\": 100,\n",
        "                  \"pca_vgg16_feature\": 200,\n",
        "                  \"pca_fourier_feature\": 25,\n",
        "                  \"pca_bag_of_visual_words\": 40\n",
        "                 }\n",
        "\n",
        "features = list(features_count.keys())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8Cj-40hEucH4",
        "outputId": "a62006ff-8708-44d4-e5a9-be59911c36c6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 222 ms (started: 2023-12-11 10:56:32 +00:00)\n"
          ]
        }
      ],
      "source": [
        "# Training set\n",
        "\n",
        "X_train = np.hstack((train_df[\"mean_h\"].to_numpy().reshape(-1, 1),\n",
        "                     train_df[\"mean_s\"].to_numpy().reshape(-1, 1),\n",
        "                     train_df[\"mean_v\"].to_numpy().reshape(-1, 1),\n",
        "                     train_df[\"stddev_h\"].to_numpy().reshape(-1, 1),\n",
        "                     train_df[\"stddev_s\"].to_numpy().reshape(-1, 1),\n",
        "                     train_df[\"stddev_v\"].to_numpy().reshape(-1, 1),\n",
        "                     np.stack(train_df[\"pca_hsv_histogram\"].to_numpy()),\n",
        "                     np.stack(train_df[\"pca_hog_feature\"].to_numpy()),\n",
        "                     np.stack(train_df[\"pca_vgg16_feature\"].to_numpy()),\n",
        "                     np.stack(train_df[\"pca_fourier_feature\"].to_numpy()),\n",
        "                     np.stack(train_df[\"pca_bag_of_visual_words\"].to_numpy())\n",
        "                     ))\n",
        "y_train = train_df[\"category\"]\n",
        "\n",
        "scaler = StandardScaler()\n",
        "scaler.fit(X_train)\n",
        "X_train = scaler.transform(X_train)\n",
        "\n",
        "# Valid Set\n",
        "X_valid = np.hstack((valid_df[\"mean_h\"].to_numpy().reshape(-1, 1),\n",
        "                     valid_df[\"mean_s\"].to_numpy().reshape(-1, 1),\n",
        "                     valid_df[\"mean_v\"].to_numpy().reshape(-1, 1),\n",
        "                     valid_df[\"stddev_h\"].to_numpy().reshape(-1, 1),\n",
        "                     valid_df[\"stddev_s\"].to_numpy().reshape(-1, 1),\n",
        "                     valid_df[\"stddev_v\"].to_numpy().reshape(-1, 1),\n",
        "                     np.stack(valid_df[\"pca_hsv_histogram\"].to_numpy()),\n",
        "                     np.stack(valid_df[\"pca_hog_feature\"].to_numpy()),\n",
        "                     np.stack(valid_df[\"pca_vgg16_feature\"].to_numpy()),\n",
        "                     np.stack(valid_df[\"pca_fourier_feature\"].to_numpy()),\n",
        "                     np.stack(valid_df[\"pca_bag_of_visual_words\"].to_numpy())\n",
        "                     ))\n",
        "X_valid = scaler.transform(X_valid)\n",
        "\n",
        "y_valid = valid_df[\"category\"]\n",
        "\n",
        "\n",
        "# Test Set\n",
        "X_test = np.hstack(( test_df[\"mean_h\"].to_numpy().reshape(-1, 1),\n",
        "                     test_df[\"mean_s\"].to_numpy().reshape(-1, 1),\n",
        "                     test_df[\"mean_v\"].to_numpy().reshape(-1, 1),\n",
        "                     test_df[\"stddev_h\"].to_numpy().reshape(-1, 1),\n",
        "                     test_df[\"stddev_s\"].to_numpy().reshape(-1, 1),\n",
        "                     test_df[\"stddev_v\"].to_numpy().reshape(-1, 1),\n",
        "                     np.stack(test_df[\"pca_hsv_histogram\"].to_numpy()),\n",
        "                     np.stack(test_df[\"pca_hog_feature\"].to_numpy()),\n",
        "                     np.stack(test_df[\"pca_vgg16_feature\"].to_numpy()),\n",
        "                     np.stack(test_df[\"pca_fourier_feature\"].to_numpy()),\n",
        "                     np.stack(test_df[\"pca_bag_of_visual_words\"].to_numpy())\n",
        "                     ))\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "y_test = test_df[\"category\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qH_VsEzjFjvG",
        "outputId": "71411ed8-28a7-41ae-b3e9-efb1bab51bf0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "param_grid = {\n",
        "    'C': [0.01, 0.1, 1, 10],\n",
        "    'penalty': ['l1', 'l2'],\n",
        "    'max_iter': [100, 200, 500]\n",
        "}\n",
        "\n",
        "lr = LogisticRegression(multi_class=\"multinomial\",\n",
        "                        solver=\"saga\")\n",
        "\n",
        "scoring = {\"accuracy\": make_scorer(accuracy_score),\n",
        "           \"precision\": make_scorer(precision_score, average='macro'),\n",
        "           \"recall\": make_scorer(recall_score, average='macro'),\n",
        "           \"f1\": make_scorer(f1_score, average='macro')}\n",
        "\n",
        "grid_search = GridSearchCV(lr, param_grid, cv=5, scoring=scoring, refit=\"f1\")\n",
        "grid_search.fit(X_train, y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QmBj-xS69ce8"
      },
      "outputs": [],
      "source": [
        "# fit_time\n",
        "cv_results = grid_search.cv_results_\n",
        "\n",
        "results_dict = {\n",
        "    \"params\" : cv_results[\"params\"],\n",
        "    \"mean_fit_time\" : cv_results[\"mean_fit_time\"],\n",
        "    \"mean_test_accuracy\": cv_results[\"mean_test_accuracy\"],\n",
        "    \"mean_test_precision\": cv_results[\"mean_test_precision\"],\n",
        "    \"mean_test_recall\" : cv_results[\"mean_test_recall\"],\n",
        "    \"mean_test_f1\" : cv_results[\"mean_test_f1\"],\n",
        "}\n",
        "\n",
        "cv_results_dict = pd.DataFrame.from_dict(results_dict)\n",
        "print(cv_results_dict)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oqUMNbgcxps2"
      },
      "outputs": [],
      "source": [
        "# getting valid set result\n",
        "\n",
        "y_valid_pred = grid_search.predict(X_valid)\n",
        "valid_df['pred'] = y_valid_pred\n",
        "\n",
        "print(classification_report(y_valid, y_valid_pred))\n",
        "\n",
        "accuracy = grid_search.score(X_valid, y_valid)\n",
        "print(f\"accuracy: {accuracy}\")\n",
        "\n",
        "confusion_matrix = metrics.confusion_matrix(y_valid, y_valid_pred)\n",
        "\n",
        "plt.figure(figsize=(9,9))\n",
        "sns.heatmap(confusion_matrix, annot=True, fmt=\"0\", linewidths=.5,\n",
        "            square = True, cmap = \"Blues\");\n",
        "plt.ylabel(\"Actual label\")\n",
        "plt.xlabel(\"Predicted label\")\n",
        "plt.xticks(ticks=np.arange(6) + 0.5, labels=grid_search.classes_, rotation=45, ha='right')\n",
        "plt.yticks(ticks=np.arange(6) + 0.5, labels=grid_search.classes_, rotation=0)\n",
        "\n",
        "all_sample_title = \"Confusion Matrix\"\n",
        "plt.title(all_sample_title, size = 15)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KRFHoccEfCq7"
      },
      "outputs": [],
      "source": [
        "coefficients = grid_search.best_estimator_.coef_\n",
        "\n",
        "feature_names = list(features_count.keys())\n",
        "\n",
        "\n",
        "feature_idx = 0\n",
        "label_idx = 0\n",
        "abs_coefficients = np.abs(coefficients)\n",
        "num_features = coefficients.shape[1]\n",
        "\n",
        "feature_importance_dict = {}\n",
        "\n",
        "while feature_idx < num_features:\n",
        "    feature_label = feature_names[label_idx]\n",
        "    num_cols = features_count[feature_label]\n",
        "    feature_importance_dict[feature_label] = np.mean(abs_coefficients[:,feature_idx: feature_idx + num_cols])\n",
        "    feature_idx += num_cols\n",
        "    label_idx += 1\n",
        "\n",
        "\n",
        "# feature importance\n",
        "sorted(feature_importance_dict.items(), key=lambda item: item[1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R0ELqx6uJ1L9"
      },
      "outputs": [],
      "source": [
        "error_df = test_df[test_df[\"category\"] != test_df[\"pred\"]]\n",
        "error_df = error_df.groupby([\"category\", \"pred\"]).head(3).reset_index()\n",
        "\n",
        "num_rows = len(error_df) // 3\n",
        "if len(error_df) % 3 != 0:\n",
        "    num_rows += 1\n",
        "\n",
        "fig, axs = plt.subplots(num_rows, 3, figsize = (12, 4 * num_rows))\n",
        "\n",
        "for idx, row in error_df.iterrows():\n",
        "    img = Image.fromarray(row[\"image\"])\n",
        "    axs[idx // 3][idx % 3].imshow(img)\n",
        "    axs[idx // 3][idx % 3].set_title(f\"actual:{row['category']} pred:{row['pred']}\")\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "machine_shape": "hm",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}